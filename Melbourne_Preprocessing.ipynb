{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the information from the JSON file into a CSV\n",
    "# Author of code: Samsung Lim\n",
    "# Date: 17th October 2021\n",
    "# Modified by Kathleen Shalini Rome\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# all\n",
    "df_all = pd.read_json('tweets_melb_earthquake_test.json', encoding='utf-8', lines=True)\n",
    "\n",
    "df_all.to_csv('Melb__all.csv', encoding='utf-8')\n",
    "\n",
    "# Extracting column data into CSV files.\n",
    "# user\n",
    "user = pd.json_normalize(df_all['user'])\n",
    "user.to_csv('Melb__user.csv', encoding='utf-8')\n",
    "\n",
    "#entities\n",
    "entities = pd.json_normalize(df_all['entities'])\n",
    "entities.to_csv('Melb__entities.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deal with the Type error - argument of type ;float; is not iterable for extended tweet \n",
    "# Source: https://coderedirect.com/questions/166716/how-to-json-normalize-a-column-with-nans\n",
    "def flatten_json(nested_json, exclude=['']):\n",
    "    \"\"\"Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "            exclude: Keys to exclude from output.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name='', exclude=exclude):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                if a not in exclude: flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out\n",
    "\n",
    "extended_tweets = pd.DataFrame([flatten_json(x) for x in df_all['extended_tweet']])\n",
    "\n",
    "extended_tweets.to_csv('Melb__extended_tweet.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 107286 entries, 0 to 107285\n",
      "Columns: 733 entries,  to extended_entities_media_3_description\n",
      "dtypes: float64(373), object(360)\n",
      "memory usage: 600.0+ MB\n"
     ]
    }
   ],
   "source": [
    "extended_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_merged = pd.concat([df_all, user, entities, extended_tweets], axis=1)\n",
    "\n",
    "# df_col_merged.to_csv('Seroja__col_merged.csv', encoding='utf-8')\n",
    " # Can improve it by adding user.id etc to make it more specific where it came from. \n",
    "#df_col_merged.to_csv('Seroja__2_col_merged.csv', encoding='utf-8')\n",
    "df_col_merged.to_csv('Melb__col_merged.csv', encoding='utf-8') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I had to manually delete the extra lang col from the user col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sy6sh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3170: DtypeWarning: Columns (37,152,153,154,155,156,157,160,163,166,169,174,175,176,177,178,179,182,185,188,191,196,197,198,199,200,201,204,207,210,213,218,219,220,221,222,223,226,229,232,235,240,241,242,243,244,245,248,251,254,257,262,263,264,265,266,267,270,273,276,279,288,289,290,296,302,311,314,317,320,323,326,329,332,335,338,341,344,347,350,353,356,357,359,360,361,362,363,366,367,369,370,371,372,379,382,385,388,391,400,401,406,407,412,413,418,419,424,425,430,431,436,437,442,443,448,449,454,455,460,461,466,467,472,473,478,479,484,485,490,491,496,497,502,503,508,509,514,515,520,521,526,527,532,533,538,539,544,545,550,551,556,557,562,563,568,569,574,575,580,581,586,587,592,593,598,599,604,605,610,611,616,617,622,623,628,629,634,635,640,641,646,647,652,653,658,659,664,665,670,671,676,677,684,689,690,692,693,695,696,697,698,703,708,709,711,712,714,715,716,717,746,749,752,755,758,759,760,761,762,763,764,765,766,769,770,771,774,777,780,783,786,789,792,795,796,797,800,803,806,807,808,809,810,813,814) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_col_merged = pd.read_csv('Melb__col_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete duplicate Tweets and tweets that dont have 'en' (english) specified in their lang field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates incase multiple retweets of the same retweet:\n",
    "df_col_merged.drop_duplicates(subset='text',inplace=True)\n",
    "# Removing non english tweets\n",
    "# Source: https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value\n",
    "df_col_merged.drop(df_col_merged.loc[df_col_merged['lang']!='en'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62851 entries, 0 to 107284\n",
      "Columns: 815 entries, Unnamed: 0 to extended_entities_media_3_description\n",
      "dtypes: bool(13), float64(464), int64(10), object(328)\n",
      "memory usage: 385.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_col_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41767 entries, 1 to 107284\n",
      "Columns: 815 entries, Unnamed: 0 to extended_entities_media_3_description\n",
      "dtypes: bool(13), float64(464), int64(10), object(328)\n",
      "memory usage: 256.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_col_merged.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract only the key featured columns required for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_2 = ['created_at','id','text','source','name', 'screen_name','location', 'description', 'followers_count', 'friends_count', 'geo_enabled', 'time_zone','geo','coordinates','place','full_text', 'timestamp_ms','lang']\n",
    "\n",
    "# Just considering 'entities' is useful to have them extracted from the text already. So might want to have those as well. \n",
    "\n",
    "df_filtered = df_col_merged[cols_2]\n",
    "# Saving the reduced dataframe to a csv\n",
    "df_filtered.to_csv('Melb_filtered_cols.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41767 entries, 1 to 107284\n",
      "Data columns (total 18 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   created_at       41767 non-null  object \n",
      " 1   id               41767 non-null  float64\n",
      " 2   text             41767 non-null  object \n",
      " 3   source           41767 non-null  object \n",
      " 4   name             41767 non-null  object \n",
      " 5   screen_name      41767 non-null  object \n",
      " 6   location         28963 non-null  object \n",
      " 7   description      37563 non-null  object \n",
      " 8   followers_count  41767 non-null  int64  \n",
      " 9   friends_count    41767 non-null  int64  \n",
      " 10  geo_enabled      41767 non-null  bool   \n",
      " 11  time_zone        0 non-null      float64\n",
      " 12  geo              2089 non-null   object \n",
      " 13  coordinates      2089 non-null   object \n",
      " 14  place            2383 non-null   object \n",
      " 15  full_text        14183 non-null  object \n",
      " 16  timestamp_ms     41767 non-null  object \n",
      " 17  lang             41767 non-null  object \n",
      "dtypes: bool(1), float64(2), int64(2), object(13)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text\n",
    "Removing urls, @ mentions and Retweets, hashtags, audio and video tags, double space, strip punctuation, remove numbers???\n",
    "Tokenise\n",
    "Lemamtize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/bicachu/topic-modeling-health-tweets/blob/master/notebooks/clean_tokenizer.py\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@'         # define a string of punctuation symbols\n",
    "\n",
    "# Functions to clean tweets\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', str(tweet))   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', str(tweet)) # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', str(tweet))\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', str(tweet))  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', str(tweet))  # remove 'AUDIO:' from start of tweet\n",
    "    return tweet\n",
    "\n",
    "def lemmatize(token):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(tweet):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS \\\n",
    "                and len(token) > 2:  # drops words with less than 3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "def basic_clean(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def shal_clean(tweet):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', str(tweet))  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', str(tweet))  # remove double spacing\n",
    "    # tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', str(tweet))\n",
    "    tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "def tokenize_tweets(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # df['tokens'] = df.tweet.apply(preprocess_tweet)\n",
    "    df['tokens'] = df['text'].apply(shal_clean)\n",
    "\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been cleaned and tokenized : {}'.format(num_tweets))\n",
    "    return df\n",
    "\n",
    "def tokenize_user_desc(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # df['tokens'] = df.tweet.apply(preprocess_tweet)\n",
    "    df ['tokens_desc'] = df['description'].apply(shal_clean)\n",
    "\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of descriptions that have been cleaned and tokenized : {}'.format(num_tweets))\n",
    "    return df\n",
    "\n",
    "def tokenize_full_text(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # df['tokens'] = df.tweet.apply(preprocess_tweet)\n",
    "    df ['tokens_full_text'] = df['full_text'].apply(shal_clean)\n",
    "\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of full texts that have been cleaned and tokenized : {}'.format(num_tweets))\n",
    "    return df\n",
    "\n",
    "# ERROR: \n",
    "# C:\\Users\\sy6sh\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
    "# warnings.warn(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been cleaned and tokenized : 41767\n",
      "Complete. Number of Tweets that have been cleaned and tokenized : 41767\n",
      "Complete. Number of Tweets that have been cleaned and tokenized : 41767\n"
     ]
    }
   ],
   "source": [
    "df_filtered = pd.read_csv('Melb_filtered_cols.csv', encoding = 'utf-8')\n",
    "df_filtered = tokenize_tweets(df_filtered)\n",
    "df_filtered = tokenize_user_desc(df_filtered)\n",
    "df_filtered = tokenize_full_text(df_filtered)\n",
    "# To account for this error - TypeError: expected string or bytes-like object - the tokenize tweet clean tweets classes were updated to make the tweet field a string. https://stackoverflow.com/questions/43727583/re-sub-erroring-with-expected-string-or-bytes-like-object\n",
    "\n",
    "df_filtered.to_csv('Melb__tokenised.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Number of Tweets that have been cleaned and tokenized : 41767\n"
     ]
    }
   ],
   "source": [
    "df_filtered_user_desc = pd.read_csv('Melb_filtered_cols.csv', encoding = 'utf-8')\n",
    "\n",
    "df_filtered_user_desc = tokenize_tweets(df_filtered_user_desc)\n",
    "# To account for this error - TypeError: expected string or bytes-like object - the tokenize tweet clean tweets classes were updated to make the tweet field a string. https://stackoverflow.com/questions/43727583/re-sub-erroring-with-expected-string-or-bytes-like-object\n",
    "\n",
    "df_filtered_user_desc.to_csv('Melb__user_desc_tokenised.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing the file into training, test and validation sets\n",
    "Breaking it down into 7000 tweets to run through geograpy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/36445193/splitting-one-csv-into-multiple-files\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def split(filehandler, delimiter=',', row_limit=15000:\n",
    "          output_name_template='output_%s.csv', output_path='.', keep_headers=True):\n",
    "    import csv\n",
    "    reader = csv.reader(filehandler, delimiter=delimiter)\n",
    "    current_piece = 1\n",
    "    current_out_path = os.path.join(\n",
    "        output_path,\n",
    "        output_name_template % current_piece\n",
    "    )\n",
    "    current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "    current_limit = row_limit\n",
    "    if keep_headers:\n",
    "        headers = reader.next()\n",
    "        current_out_writer.writerow(headers)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i + 1 > current_limit:\n",
    "            current_piece += 1\n",
    "            current_limit = row_limit * current_piece\n",
    "            current_out_path = os.path.join(\n",
    "                output_path,\n",
    "                output_name_template % current_piece\n",
    "            )\n",
    "            current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "            if keep_headers:\n",
    "                current_out_writer.writerow(headers)\n",
    "        current_out_writer.writerow(row)\n",
    "\n",
    "split(open('/your/pat/input.csv', 'r'));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open('Melb__tokenised.csv', 'r', encoding='utf-8').readlines()\n",
    ">>> filename = 1\n",
    ">>> for i in range(len(csvfile)):\n",
    "...     if i % 14000 == 0: #Dividing 40,000 tweets into 3 files approximately the same size. \n",
    "...         open(str(filename) + '.csv', 'w+', encoding='utf-8').writelines(csvfile[i:i+1000])\n",
    "...         filename += 1\n",
    "\n",
    "# Resulted in 8 460 lined excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 2859: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-412ad1287f5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#get the number of lines of the csv file to be read\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnumber_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-412ad1287f5d>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#get the number of lines of the csv file to be read\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnumber_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 2859: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#csv file name to be read in \n",
    "\n",
    "in_csv = 'Melb__tokenised.csv'\n",
    "\n",
    "\n",
    "#get the number of lines of the csv file to be read\n",
    "\n",
    "number_lines = sum(1 for row in (open(in_csv)))\n",
    "\n",
    "\n",
    "#size of rows of data to write to the csv, \n",
    "\n",
    "#you can change the row size according to your need\n",
    "\n",
    "rowsize = 7000\n",
    "\n",
    "\n",
    "#start looping through data writing it to a new file for each set\n",
    "\n",
    "for i in range(1,number_lines,rowsize):\n",
    "\n",
    "    df = pd.read_csv(in_csv,\n",
    "\n",
    "          header=None,\n",
    "\n",
    "         nrows = rowsize,#number of rows to read at each loop\n",
    "\n",
    "          skiprows = i)#skip rows that have been read\n",
    "\n",
    "\n",
    "    #csv to write data to a new file with indexed name. input_1.csv etc.\n",
    "\n",
    "    out_csv = 'input' + str(i) + '.csv'\n",
    "\n",
    "\n",
    "    df.to_csv(out_csv,\n",
    "\n",
    "          index=False,\n",
    "\n",
    "          header=False,\n",
    "\n",
    "          mode='a',#append data to csv file\n",
    "\n",
    "          chunksize=rowsize)#size of data to append for each loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geograpy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geograpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-221608d8f0e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeograpy3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgeograpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Melb__tokenised.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geograpy'"
     ]
    }
   ],
   "source": [
    "import geograpy3\n",
    "import pandas as pd\n",
    "\n",
    "df_filtered = pd.read_csv(\"Melb__tokenised.csv\")\n",
    "\n",
    "df_filtered['place_context'] = df_filtered['tokens'].apply(geograpy.get_place_context(text = 'tokens'))  \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44f4afc50692ecda004cf5be1955afd0497ef131fe57a1dc393b129dd3963ae0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
